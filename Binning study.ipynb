{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning SNR and Chastity data to generate Qtable \n",
    "\n",
    "**Concept**\n",
    "<br>\n",
    "The  main concept of this study is to utilize the approach of binning the SNR and chastity data to generate Qscore (or Qtable) of each channel (A, T, G, C) at each cycle. Qscore is the quality score of basecall. In this study, three variables were used and they are SNR, chastity and cycle. Qscore is generated from error rate and error rate is determined by bad reads/(good reads + bad reads). The definition of good and bad reads in this study is defined as aligned and not aligned reads, respectively. The files used for determining if the reads are aligned are the perfect match files. \n",
    "\n",
    "There are two studies in this file. The first study (section 2) is binning the data of selected cycles and this section of code was developed to do the binning of multiple experiments so that a lot of data were used to generate the Qtable for cycle 21 and 50. The second study (section 3) is binning the data from cycle 21 to 150. The first 20 cycles were excluded due to many junk clusters being presented. Only one experiment was conducted in the second approach due to the lengthy computational time. \n",
    "\n",
    "There is only one method to bin the chastity data but there are two binning methods for SNR. The reason why there are two binning methods for SNR is because SNR data has not been processed in a way that the data is relative to other channels. On the other hand, chastity data has been normalized and compared to the other channels; therefore, binning chastity uses the raw data generated from the bioinformatics pipeline. For SNR, the two binning methods are based on (1) normalized values and (2) basecall normalized values minus the next maximum value (max_minus). The normalized values are equal to raw values divided by the median of the raw values. The following example illustrates how the max_minus approach is done: let assume a cluster at a particular cycle has the normalized SNR values as A = 2.5, T= 3.5, G = 3.4, C = 2.8. The basecall for that cycle is G. In this example, the max_minus will be 3.4 (basecall normalized value) - 3.5 (next maximum value) = -0.1.\n",
    "\n",
    "The binning methodology used in this study is different from the most common binning method, which is based on dividing the range of values in the data set to the equal section. For example, if a data set ranges from 0 to 100 and the bin edges will be 0, 20, 40, 60, 80 and 100 to generate five bins in this data set. However, the binning method of this study is based on equal population. For example, a data set has 100 data points and to generate 5 bins, the data set will first need to be sorted in the ascending order and the values corresponding to the positions (or indexes) of 1, 20, 40, 60, 80 and 100 in this data set are the bin edges. Here are the outlines for this file:\n",
    "1. Import the python packages\n",
    "2. Binning SNR and chastity data of selected cycles<br>\n",
    "    2a. Function scripts<br>\n",
    "    2b. Get bin edges<br>\n",
    "    2c. Generate Qtable<br>\n",
    "3. Binning all CRT87x SNR and CRFL data<br>\n",
    "    3a. Function scripts<br>\n",
    "    3b. Get bin edges<br>\n",
    "    3c. Generate Qtable<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Import the python packages\n",
    "First of all, import all athe requried python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, re, sys, operator, glob,math\n",
    "import statistics as stat\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from pathlib import Path\n",
    "import natsort as ns\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Binning SNR and chastity data of selected cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Function scripts\n",
    "\n",
    "Function scripts for further analysis. Just run the cell and don't need to change anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(path, pattern):\n",
    "    files_in_dir = []\n",
    "    # r=>root, d=>directories, f=>files\n",
    "    for r, d, f in os.walk(path):\n",
    "       for item in f:\n",
    "          if pattern in item:\n",
    "             files_in_dir.append(os.path.join(r, item))\n",
    "    return files_in_dir\n",
    "\n",
    "def Bin_Equal_Count(dblist, Base, cycle, num_bin = 50):\n",
    "    histdb = pd.concat(dblist).sort_values()\n",
    "    rowvalue = len(histdb)/num_bin\n",
    "    \n",
    "    row = 0\n",
    "    bin_edge = []\n",
    "    while row < num_bin:\n",
    "        rows = round(rowvalue*row)\n",
    "        bin_edge.append(histdb.iloc[rows, ])\n",
    "        row = row + 1\n",
    "        \n",
    "    bin_edge.append(histdb.iloc[(len(histdb))-1,])\n",
    "    binname = Base + str(cycle)\n",
    "    binedge_dict =  {binname : bin_edge}\n",
    "    return binedge_dict\n",
    "\n",
    "def dictionOne(L):\n",
    "    result = {}\n",
    "    for d in L:\n",
    "        result.update(d)\n",
    "    return result\n",
    "\n",
    "def histdict(db, cycles = [21, 50]):\n",
    "    hist_ID_list = []\n",
    "    for B in ['A', 'T', 'G', 'C']:\n",
    "        for i in cycles:\n",
    "            hist_ID_list.append(B + str(i))\n",
    "            \n",
    "    hist_bin = { i : [] for i in hist_ID_list}\n",
    "    \n",
    "    for ID in hist_ID_list:\n",
    "        x = db[ID].tolist()\n",
    "        for i in range(0, len(x)-1):\n",
    "            hist_bin[ID].append((x[i], x[i + 1]))\n",
    "    return hist_bin\n",
    "\n",
    "def make_folder(save_folder):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Get bin edges\n",
    "\n",
    "The following cell is for files and parameter input. Here are the description of each input:\n",
    "1. experiment_folder_path = the folder path where all the raw data is stored.\n",
    "2. save_edges_folder = the bin edges files will be saved\n",
    "2. file_type_list = there should be six file types. When this study was conducted, these files had the following file extensions:<br>\n",
    "    A. id.Base = it contains cluster ID, cycle number, basecall and the column headings are [ID, Cycle, Base]<br>\n",
    "    B. id.miss = it contains cluster ID and the cycle numbers where the clusters become no longer perfect aligned to the reference genome. The column headings are [ID, Miss]<br>\n",
    "    C. id.intensity\", \"id.score\", \"id.Signal\", \"id.SNR = background fluorescence intensity (intensity), chasity (score), relative fluorescence intensity (Signal), signal to noise ratio (SNR). The column headings are [ID, Cycle, A,\tT, G, C]<br>\n",
    "3. selected_cycle = enter the cycle numbers that will be analyzed as a list (e.g. [21, 50])\n",
    "4. num_bin = numbers of bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the file paths\n",
    "experiment_folder_path = \"D:/Binning_study_KW/Example/\"\n",
    "save_edges_folder = experiment_folder_path  + \"Bin_edges/\"\n",
    "filetypelist = [\"id.Base\", \"id.intensity\", \"id.score\", \"id.Signal\", \"id.SNR\", \"miss\"]\n",
    "selected_cycle = [21, 50]\n",
    "num_bin = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells are the scripts to generate chastity and SNR bins. Three files will be generated after running these two scripts and they are the bin edges files for chastity, normalized SNR, and max_minus of normalized SNR. These files will be in a Bin_edges folder inside the folder defined in the experiment_folder_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get bin edges for chastity \n",
    "\n",
    "#input the data path\n",
    "experiment_folder_path = experiment_folder_path\n",
    "\n",
    "#generate the file list \n",
    "filetypelist = filetypelist\n",
    "goodfiletype= [\"good.\" + i for i in filetypelist[0:5]]\n",
    "\n",
    "# Bin for chastity\n",
    "goodfile = list_files(path = experiment_folder_path , pattern = goodfiletype[2])\n",
    "goodID = list_files(path = experiment_folder_path , pattern = goodfiletype[0])\n",
    "\n",
    "#get bin edge values\n",
    "chas_value_dict_list = []\n",
    "for B in ['A', 'T', 'G', 'C']:  \n",
    "    for cyc in selected_cycle:\n",
    "        value_list = []\n",
    "        for file in range(0, len(goodfile)):\n",
    "        \n",
    "            IDdb = pd.read_csv(goodID[file], sep = '\\t')\n",
    "            db = pd.read_csv(goodfile[file], sep = '\\t')\n",
    "            \n",
    "            db1 = pd.merge(IDdb, db, on = [\"ID\", \"Cycle\"], how = \"inner\")\\\n",
    "                            .rename(columns = {'Base' : 'call_base'})\n",
    "                            \n",
    "            db1 = db1[(db1['Cycle'] == cyc) & (db1['call_base'] == B)] \n",
    "            value_list.append(db1[B])              \n",
    "        #create dictionary and append to value_dict_list            \n",
    "        value_dict = Bin_Equal_Count(dblist = value_list, \n",
    "                                 cycle= cyc, \n",
    "                                 Base = B, num_bin = num_bin)\n",
    "        \n",
    "        chas_value_dict_list.append(value_dict)\n",
    "\n",
    "#convert bin edges list to bin edges dataframe and save the folder        \n",
    "chasdb = pd.DataFrame(dictionOne(chas_value_dict_list))\n",
    "save_folder = save_edges_folder\n",
    "make_folder(save_folder)\n",
    "chasdb.to_csv(save_folder + 'Chastity_bin_edges_by_cycles_equal_count_' + str(num_bin) + '.tsv',\n",
    "       sep='\\t', index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get bin edges for SNR\n",
    "\n",
    "#input the data path\n",
    "experiment_folder_path = experiment_folder_path\n",
    "\n",
    "filetypelist = filetypelist\n",
    "goodfiletype= [\"good.\" + i for i in filetypelist[0:5]]\n",
    "\n",
    "goodfile = list_files(path = experiment_folder_path, pattern = goodfiletype[4])\n",
    "goodID = list_files(path = experiment_folder_path, pattern = goodfiletype[0])\n",
    "\n",
    "value_dict_list = []\n",
    "max_dict_list = []\n",
    "\n",
    "#get bin edge values\n",
    "for B in ['A', 'T', 'G', 'C']:  \n",
    "    for cyc in [21, 50]:\n",
    "        value_list = []\n",
    "        max_list = []\n",
    "        for file in range(0, len(goodfile)):\n",
    "        \n",
    "            IDdb = pd.read_csv(goodID[file], sep = '\\t')\n",
    "            db = pd.read_csv(goodfile[file], sep = '\\t')\n",
    "            \n",
    "            db1 = pd.merge(IDdb, db, on = [\"ID\", \"Cycle\"], how = \"inner\")\\\n",
    "                            .rename(columns = {'Base' : 'call_base'})\n",
    "                            \n",
    "            db1['A'] =  db1['A']/stat.median( db1['A'])\n",
    "            db1['T'] =  db1['T']/stat.median( db1['T'])\n",
    "            db1['G'] =  db1['G']/stat.median( db1['G'])\n",
    "            db1['C'] =  db1['C']/stat.median( db1['C'])\n",
    "            #To calculate the normalized value and append the values to value_list                \n",
    "            db1 = db1[(db1['Cycle'] == cyc) & (db1['call_base'] == B)]                \n",
    "            value_list.append(db1[B])\n",
    "            \n",
    "            #To calculate the maxminus value and append the values to max_list\n",
    "            secondmax = db1[B] - db1[['A', 'T', 'G', 'C']]\\\n",
    "                                            .drop([B], axis=1)\\\n",
    "                                            .max(axis=1) \n",
    "            max_list.append(secondmax)\n",
    "        \n",
    "        #create dictionary and append to value_dict_list\n",
    "        value_dict = Bin_Equal_Count(dblist = value_list, \n",
    "                                     cycle= cyc, \n",
    "                                     Base = B, num_bin = num_bin)\n",
    "        #create maxminus dictionary and append to value_dict_list\n",
    "        max_dict = Bin_Equal_Count(dblist = max_list, \n",
    "                                     cycle= cyc, \n",
    "                                     Base = B, num_bin = num_bin)\n",
    "        \n",
    "        value_dict_list.append(value_dict)\n",
    "        max_dict_list.append(max_dict)\n",
    "#concatenate dictionary to dataframe and save the bin edges\n",
    "SNRvaluedb = pd.DataFrame(dictionOne(value_dict_list))\n",
    "SNRmaxdb = pd.DataFrame(dictionOne(max_dict_list))\n",
    "make_folder(save_folder)\n",
    "save_folder = save_edges_folder\n",
    "\n",
    "make_folder(save_folder)\n",
    "SNRvaluedb.to_csv(save_folder + 'SNR_value_bin_edges_by_cycles_equal_count_' + str(num_bin) + '.tsv',\n",
    "       sep='\\t', index=False)  \n",
    "\n",
    "SNRmaxdb.to_csv(save_folder + 'SNR_max_diff_bin_edges_by_cycles_equal_count_' + str(num_bin) + '.tsv',\n",
    "       sep='\\t', index=False)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Generate Qtable\n",
    "\n",
    "After getting the bin edges, run the following two cells to generate the Qtable. Here are the path information:\n",
    "1. experiment_folder_path = it should be the same as the experiment_folder_path in the section (2a-2), where all the data is stored\n",
    "2. result_folder = it is the folder you want to store your results\n",
    "3. SNR_bin_edge_path = it is the SNR bin edges files generated using normalized value\n",
    "4. SNR_bin_edge_max_path = it is the SNR bin edges files generated using maxminus method\n",
    "5. SNR_bin_edge_max_path = it is the chastity bin edges files\n",
    "6. selected cycles = the cycles will be analyzed and in this particular study, the cycles are [21, 50]\n",
    "7. chastity_reduce_bin = the numbers of chastity bin will be compressed into one bin \n",
    "8. SNR_reduce_bin = the numbers of SNR bin will be compressed into one bin  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input path, selected_cycle, number of chastity and SNR bin\n",
    "experiment_folder_path = \"D:/Binning_study_KW/Example/\"\n",
    "result_folder = 'D:/Binning_study_KW/Example/Result/'\n",
    "SNR_bin_edge_path = 'D:/Binning_study_KW/Example/Bin_edges/SNR_value_bin_edges_by_cycles_equal_count_30.tsv'\n",
    "SNR_bin_edge_max_path = 'D:/Binning_study_KW/Example/Bin_edges/SNR_max_diff_bin_edges_by_cycles_equal_count_30.tsv'\n",
    "chasity_bin_edge_path = 'D:/Binning_study_KW/Example/Bin_edges/Chastity_bin_edges_by_cycles_equal_count_30.tsv'\n",
    "selected_cycle = [21, 50]\n",
    "chastity_reduce_bin = 10\n",
    "SNR_reduce_bin = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After entering the path and other input, run the following cell to generate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the scripts to generate the numbers of good and bad reads, error rate and Qscores\n",
    "\n",
    "#load the raw data files\n",
    "filetypelist = [\"id.Base\", \"id.intensity\", \"id.score\", \"id.Signal\", \"id.SNR\", \"miss\"]\n",
    "\n",
    "goodfiletype= [\"good.\" + i for i in filetypelist[0:5]]\n",
    "badfiletype= [\"bad.\" + i for i in filetypelist]\n",
    "\n",
    "goodchasfile = list_files(path = experiment_folder_path, pattern = goodfiletype[2])\n",
    "badchasfile = list_files(path = experiment_folder_path, pattern = badfiletype[2])\n",
    "\n",
    "goodSNRfile = list_files(path = experiment_folder_path, pattern = goodfiletype[4])\n",
    "badSNRfile = list_files(path = experiment_folder_path, pattern = badfiletype[4])\n",
    "\n",
    "goodID = list_files(path = experiment_folder_path, pattern = goodfiletype[0])\n",
    "badID = list_files(path = experiment_folder_path, pattern = badfiletype[0])\n",
    "\n",
    "badmissfile = list_files(path = experiment_folder_path, pattern = badfiletype[5])\n",
    "\n",
    "filename = list(map(lambda x: re.findall(r\"CRT\\d+x_Inc_\\w\\d\\d\\.\\d\\d\", x), goodID))\n",
    "filename = [i[0] for i in filename]\n",
    "\n",
    "\n",
    "#load bin edges files\n",
    "SNR_db = pd.read_csv(SNR_bin_edge_path, sep = \"\\t\")\n",
    "SNR_max_db = pd.read_csv(SNR_bin_edge_max_path, sep = \"\\t\")\n",
    "chas_db = pd.read_csv(chasity_bin_edge_path, sep = \"\\t\")\n",
    "\n",
    "# this dictionary is for looping later\n",
    "SNR_hist_bin = {'SNR_value' : SNR_db, 'maxminus' : SNR_max_db}\n",
    "\n",
    "db_dict = {}\n",
    "for file in range(0, len(goodID)):\n",
    "    Good_chas_list = []\n",
    "    bad_chas_list_good = []\n",
    "    bad_chas_list_bad = []\n",
    "    Good_SNR_list = []\n",
    "    bad_SNR_list_good = []\n",
    "    bad_SNR_list_bad = []\n",
    "    \n",
    "    #Read the good and bad chastity files and rearange the data \n",
    "    for cyc in [21, 50]:\n",
    "    \n",
    "        #read the good_chastity, good ID files   \n",
    "        IDdb = pd.read_csv(goodID[file], sep = '\\t')\n",
    "        db = pd.read_csv(goodchasfile[file], sep = '\\t')\n",
    "        \n",
    "        db1 = pd.merge(IDdb, db, on = [\"ID\", \"Cycle\"], how = \"inner\")\\\n",
    "                        .rename(columns = {'Base' : 'call_base'})\n",
    "                        \n",
    "        db1 =  db1[db1.Cycle == cyc]     \n",
    "                        \n",
    "        db1['chas_value'] = db1.lookup(db1.index, db1['call_base'].astype(str))\n",
    "        db1 = db1[['ID', 'Cycle', 'call_base', 'chas_value']]\n",
    "        db1['type'] = 0 #all data is aligned (0)\n",
    "        \n",
    "        Good_chas_list.append(db1)\n",
    "        \n",
    "        #read the bad chastity and ID files and miss files    \n",
    "        IDdb = pd.read_csv(badID[file], sep = '\\t')\n",
    "        db = pd.read_csv(badchasfile[file], sep = '\\t')\n",
    "        missdb = pd.read_csv(badmissfile[file], sep = '\\t')\n",
    "        \n",
    "        db1 = pd.merge(IDdb, db, on = [\"ID\", \"Cycle\"], how = \"inner\")\\\n",
    "                        .rename(columns = {'Base' : 'call_base'})\n",
    "        # remove all the reads contains N                \n",
    "        db1 =  db1[(db1.call_base != \"N\") & (db1.Cycle == cyc)]                \n",
    "                        \n",
    "        db1['chas_value'] = db1.lookup(db1.index, db1['call_base'].astype(str))\n",
    "        \n",
    "        #this script is to label the ID as aligned (0) and not aligned (1) for cycle 21 and 50\n",
    "        if cyc == 21:\n",
    "            missdb.loc[missdb['Miss'] == 21, 'type'] = 1\n",
    "            missdb.loc[missdb['Miss'] > 21, 'type'] = 0\n",
    "            \n",
    "            db2 = pd.merge(db1, missdb[['ID', 'type']], on = [\"ID\"], \n",
    "                           how = \"inner\")\\\n",
    "                            [['ID', 'Cycle', 'call_base', 'chas_value', 'type']] \n",
    "                            \n",
    "            bad_chas_list_good.append(db2[db2['type'] == 0])\n",
    "            bad_chas_list_bad.append(db2[db2['type'] == 1])\n",
    "            \n",
    "        elif cyc == 50:\n",
    "            missdb = missdb[missdb.Miss == 50].assign(type  = 1)\n",
    "            \n",
    "            db2 = pd.merge(db1, missdb[['ID', 'type']], on = [\"ID\"], \n",
    "                           how = \"inner\")\\\n",
    "                            [['ID', 'Cycle', 'call_base', 'chas_value', 'type']]\n",
    "            bad_chas_list_bad.append(db2[db2['type'] == 1])\n",
    "            \n",
    "        #Read the good SNR adn ID files, normalized the values and calculate maxminus \n",
    "        IDdb = pd.read_csv(goodID[file], sep = '\\t')\n",
    "        db = pd.read_csv(goodSNRfile[file], sep = '\\t')\n",
    "        \n",
    "        db1 = pd.merge(IDdb, db, on = [\"ID\", \"Cycle\"], how = \"inner\")\\\n",
    "                        .rename(columns = {'Base' : 'call_base'})\n",
    "                        \n",
    "        db1 =  db1[db1.Cycle == cyc]\n",
    "        \n",
    "        db1['A'] =  db1['A']/stat.median( db1['A'])\n",
    "        db1['T'] =  db1['T']/stat.median( db1['T'])\n",
    "        db1['G'] =  db1['G']/stat.median( db1['G'])\n",
    "        db1['C'] =  db1['C']/stat.median( db1['C'])\n",
    "        \n",
    "        db1['SNR_value'] = db1.lookup(db1.index, db1['call_base'].astype(str))\n",
    "        #calculate the maxminus values\n",
    "        m = db1.columns[3:7].to_numpy() == db1['call_base'].to_numpy()[:, None]\n",
    "        db1['maxminus'] = db1['SNR_value'] - db1.drop(columns= ['ID', 'Cycle',\n",
    "                                                                'call_base', \n",
    "                                                                'SNR_value'])\\\n",
    "                                                                  .mask(m).max(1)\n",
    "        \n",
    "        \n",
    "        db1 = db1[['ID', 'Cycle', 'call_base', 'SNR_value', 'maxminus']]\n",
    "        db1['type'] = 0\n",
    "        \n",
    "        Good_SNR_list.append(db1)\n",
    "        \n",
    "        #Read the good SNR adn ID files, normalized the values and calculate maxminus   \n",
    "        IDdb = pd.read_csv(badID[file], sep = '\\t')\n",
    "        db = pd.read_csv(badSNRfile[file], sep = '\\t')\n",
    "        missdb = pd.read_csv(badmissfile[file], sep = '\\t')\n",
    "        \n",
    "        db1 = pd.merge(IDdb, db, on = [\"ID\", \"Cycle\"], how = \"inner\")\\\n",
    "                        .rename(columns = {'Base' : 'call_base'})\n",
    "                        \n",
    "        db1 =  db1[(db1.call_base != \"N\") & (db1.Cycle == cyc)]  \n",
    "\n",
    "        db1['A'] =  db1['A']/stat.median( db1['A'])\n",
    "        db1['T'] =  db1['T']/stat.median( db1['T'])\n",
    "        db1['G'] =  db1['G']/stat.median( db1['G'])\n",
    "        db1['C'] =  db1['C']/stat.median( db1['C'])\n",
    "                      \n",
    "        db1['SNR_value'] = db1.lookup(db1.index, db1['call_base'].astype(str))\n",
    "        m = db1.columns[3:7].to_numpy() == db1['call_base'].to_numpy()[:, None]\n",
    "        db1['maxminus'] = db1['SNR_value'] - db1.drop(columns= ['ID', 'Cycle',\n",
    "                                                                'call_base', \n",
    "                                                                'SNR_value'])\\\n",
    "                                                                  .mask(m).max(1)\n",
    "        #this script is to label the ID as aligned (0) and not aligned (1) for cycle 21 and 50\n",
    "        if cyc == 21:\n",
    "            missdb.loc[missdb['Miss'] == 21, 'type'] = 1\n",
    "            missdb.loc[missdb['Miss'] > 21, 'type'] = 0\n",
    "            \n",
    "            db2 = pd.merge(db1, missdb[['ID', 'type']], on = [\"ID\"], \n",
    "                           how = \"inner\")\\\n",
    "                            [['ID', 'Cycle', 'call_base', 'SNR_value', \n",
    "                              'type', 'maxminus']] \n",
    "                            \n",
    "            bad_SNR_list_good.append(db2[db2['type'] == 0])\n",
    "            bad_SNR_list_bad.append(db2[db2['type'] == 1])\n",
    "            \n",
    "        elif cyc == 50:\n",
    "            missdb = missdb[missdb.Miss == 50].assign(type  = 1)\n",
    "            \n",
    "            db2 = pd.merge(db1, missdb[['ID', 'type']], on = [\"ID\"], \n",
    "                           how = \"inner\")\\\n",
    "                            [['ID', 'Cycle', 'call_base', 'SNR_value', \n",
    "                              'type', 'maxminus']]\n",
    "            bad_SNR_list_bad.append(db2[db2['type'] == 1])\n",
    "            \n",
    "    #combine list of dataframe into one dataframe\n",
    "    SNR_good_combine_list =  Good_SNR_list + bad_SNR_list_good\n",
    "    chas_good_combine_list =  Good_chas_list + bad_chas_list_good\n",
    "    \n",
    "    SNR_good_combine_list = pd.concat(SNR_good_combine_list)\n",
    "    chas_good_combine_list = pd.concat(chas_good_combine_list)\n",
    "    \n",
    "    SNR_bad_combine_list = pd.concat(bad_SNR_list_bad)\n",
    "    chas_bad_combine_list = pd.concat(bad_chas_list_bad)\n",
    "    \n",
    "    #Adding bin ID to the combined dataframe, there are three loops: (1) it is to loop through two different SNR binning \n",
    "    #methods, (2) different bases, and (3) different cycles.\n",
    "    for s in SNR_hist_bin.keys():\n",
    "        save_list = []\n",
    "        for B in ['A', 'T', 'G', 'C']:\n",
    "            \n",
    "            bin_id_list = []\n",
    "            \n",
    "            #Adding bin ID to the combined good reads dataframe\n",
    "            for cyc in selected_cycle:\n",
    "                dfm = pd.merge(SNR_good_combine_list\\\n",
    "                               [[s, 'ID', 'Cycle']], \n",
    "                               chas_good_combine_list, \n",
    "                               on = [\"ID\", \"Cycle\"], how = \"inner\")\n",
    "                df = dfm[['ID',  'Cycle', 'call_base',  'chas_value', \n",
    "                          s ,'type']][(dfm['call_base'] == B)\\\n",
    "                                                  & (dfm['Cycle'] == cyc)]\n",
    "                \n",
    "                hist_ID = B + str(cyc)\n",
    "                \n",
    "                # create the bin ID column for chastity\n",
    "                hist_bin = chas_db[hist_ID][0: len(chas_db[hist_ID])-chastity_reduce_bin]\n",
    "                bin_edges =  hist_bin[0: (len(hist_bin)-1)].tolist()\n",
    "                bin_edges.append(float(\"inf\")) #adding inf to last bin to capture all the upper values\n",
    "                bin_edges[0] = -float(\"inf\") #adding -inf to first bin to capture all the lower values\n",
    "                values = [f'_C{i}' for i in range(1, len(bin_edges))]\n",
    "                df['chas_bin'] =  pd.cut(x = df['chas_value'], bins = bin_edges, labels= values)\n",
    "\n",
    "                # create the bin ID column for SNR\n",
    "                hist_bin = SNR_hist_bin[s][hist_ID][0: len(SNR_hist_bin[s][hist_ID]) - SNR_reduce_bin]\n",
    "                bin_edges =  hist_bin[0: (len(hist_bin)-1)].tolist()\n",
    "                bin_edges.append(float(\"inf\")) #adding inf to last bin to capture all the upper values\n",
    "                bin_edges[0] = -float(\"inf\") #adding -inf to first bin to capture all the lower values\n",
    "                values = [f'_S{i}' for i in range(1, len(bin_edges))]\n",
    "                df['SNR_bin'] = pd.cut(x = df[s], bins = bin_edges, labels= values)\n",
    "\n",
    "                df['bin_id'] = df.Cycle.astype(str) + df.chas_bin.astype(str) + df.SNR_bin.astype(str)\n",
    "                bin_id_list.append(df['bin_id'])\n",
    "                \n",
    "                bin_db = pd.concat(bin_id_list)\n",
    "                bin_dict = bin_db.value_counts().to_dict()\n",
    "                \n",
    "                ID_column = []\n",
    "                for cyc in selected_cycle:\n",
    "                    for c in range(1, len(bin_edges)):\n",
    "                        for r in range(1, len(bin_edges)):\n",
    "                            ID = str(cyc) + \"_C\" + str(c) + \"_S\" + str(r)\n",
    "                            ID_column.append(ID)\n",
    "                            \n",
    "                count_column = [0] * len(ID_column)\n",
    "                \n",
    "            Good_bin_db = pd.DataFrame({'ID' : ID_column, 'good_counts' : count_column})\n",
    "            Good_bin_db['good_counts'] = Good_bin_db['ID'].map(bin_dict).fillna(0)\n",
    "            \n",
    "            #Adding bin ID to the combined bad reads dataframe\n",
    "            bin_id_list = []\n",
    "            for cyc in [21, 50]:\n",
    "            \n",
    "                dfm = pd.merge(SNR_bad_combine_list\\\n",
    "                                    [[s, 'ID', 'Cycle']], \n",
    "                                    chas_bad_combine_list, \n",
    "                                    on = [\"ID\", \"Cycle\"], how = \"inner\")\n",
    "                df = dfm[['ID',  'Cycle', 'call_base',  'chas_value', \n",
    "                       s ,'type']][dfm.call_base == B]\n",
    "                \n",
    "                hist_ID = B + str(cyc)\n",
    "                \n",
    "                # create the bin ID column for chastity\n",
    "                hist_bin = chas_db[hist_ID][0: len(chas_db[hist_ID])- chastity_reduce_bin]\n",
    "                bin_edges =  hist_bin[0: (len(hist_bin)-1)].tolist()\n",
    "                bin_edges.append(float(\"inf\")) #adding inf to last bin to capture all the upper values\n",
    "                bin_edges[0] = -float(\"inf\") #adding -inf to first bin to capture all the lower values\n",
    "                values = [f'_C{i}' for i in range(1, len(bin_edges))]\n",
    "                df['chas_bin'] =  pd.cut(x = df['chas_value'], bins = bin_edges, labels= values)\n",
    "\n",
    "                # create the bin ID column for SNR\n",
    "                hist_bin = SNR_hist_bin[s][hist_ID][0: len(SNR_hist_bin[s][hist_ID])- SNR_reduce_bin]\n",
    "                bin_edges =  hist_bin[0: (len(hist_bin)-1)].tolist()\n",
    "                bin_edges.append(float(\"inf\")) #adding inf to last bin to capture all the upper values\n",
    "                bin_edges[0] = -float(\"inf\") #adding -inf to first bin to capture all the lower values\n",
    "                values = [f'_S{i}' for i in range(1, len(bin_edges))]\n",
    "                df['SNR_bin'] = pd.cut(x = df[s], bins = bin_edges, labels= values)\n",
    "\n",
    "                df['bin_id'] = df.Cycle.astype(str) + df.chas_bin.astype(str) + df.SNR_bin.astype(str)\n",
    "                bin_id_list.append(df['bin_id'])\n",
    "                \n",
    "                bin_db = pd.concat(bin_id_list)\n",
    "                bin_dict = bin_db.value_counts().to_dict()\n",
    "                \n",
    "                ID_column = []\n",
    "                for cyc in selected_cycle:\n",
    "                    for c in range(1, len(bin_edges)):\n",
    "                        for r in range(1, len(bin_edges)):\n",
    "                            ID = str(cyc) + \"_C\" + str(c) + \"_S\" + str(r)\n",
    "                            ID_column.append(ID)\n",
    "                            \n",
    "                count_column = [0] * len(ID_column)\n",
    "            \n",
    "            Bad_bin_db = pd.DataFrame({'ID' : ID_column, 'bad_counts' : count_column})\n",
    "            Bad_bin_db['bad_counts'] = Bad_bin_db['ID'].map(bin_dict).fillna(0)\n",
    "        \n",
    "            #merging the good and bad reads together and append the dataframes to a dictionary \n",
    "            save_bin_db = pd.merge(Bad_bin_db, Good_bin_db, \n",
    "                           on = [\"ID\"], how = \"inner\")\n",
    "            \n",
    "            result_folder = result_folder\n",
    "            make_folder(result_folder)\n",
    "            \n",
    "            list_name = filename[file] + '_' + s + '_base_' + B\n",
    "            db_dict[list_name] = save_bin_db\n",
    "            \n",
    "# This section is to process db_list and save the files. Four types of files [\"bad_counts\", \"good_counts\", \"error\", \"qscore\"]\n",
    "# at each cycle and bases will be saved. The file will be saved in the format of SNR bins as the row index and chastity bins\n",
    "# as the column headings\n",
    "chastity_name = ['chastity' + str(x) for x in range(1, len(bin_edges))]\n",
    "SNR_name = ['SNR' + str(x) for x in range(1, len(bin_edges))]\n",
    "\n",
    "for SNR_method in SNR_hist_bin.keys():\n",
    "    for base in [\"base_A\", \"base_T\", \"base_G\", \"base_C\"]:\n",
    "        L1 = [s for s in db_dict.keys() if SNR_method in s and base in s]\n",
    "        L2 = []\n",
    "        for file in L1:\n",
    "            L2.append(db_dict[file])\n",
    "        db = pd.concat(L2).groupby(by=[\"ID\"]).sum().reset_index().sort_values(by=['ID'])\n",
    "        db['ID'] = pd.Categorical(db['ID'], ordered=True, categories= ns.natsorted(db['ID'].unique()))\n",
    "        db = db.sort_values('ID')\n",
    "        db[['Cycle','chas_bin', 'SNR_bin']] = db.ID.str.split(\"_\",expand=True,)\n",
    "        db['error'] = db.bad_counts / (db.good_counts + db.bad_counts )\n",
    "        db['qscore'] = round((10 * -np.log10(db.error)), 0)\n",
    "        for cyc in selected_cycle:\n",
    "            for name in [\"bad_counts\", \"good_counts\", \"error\", \"qscore\"]:\n",
    "                db1 = db[db.Cycle == str(cyc)][name]\n",
    "                n = len(bin_edges)-1\n",
    "                outList = []\n",
    "                for i in range(n, len(db1) + n, n):\n",
    "                    outList.append(db1[i-n:i])\n",
    "\n",
    "                db2 = pd.DataFrame(np.array(outList), columns=chastity_name,\n",
    "                                  index = SNR_name)\n",
    "                savefolder = result_folder + SNR_method + '/' + \"cycle_\" + str(cyc) + \"_\" +  base +  \"/\" \n",
    "                make_folder(savefolder)\n",
    "                filename = savefolder + SNR_method + \"_\" + base + \"_cycle_\" + str(cyc) + \"_\" + name + '.csv'\n",
    "                db2.to_csv(filename, sep=',', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binning all CRT87x SNR and CRFL data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Function scripts\n",
    "\n",
    "Function scripts for further analysis. Just run the cell and don't need to change anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make histogram ID list\n",
    "def make_hist_ID(cycle):\n",
    "    hist_ID_list = []\n",
    "    for B in ['A', 'T', 'G', 'C']:\n",
    "        for i in cycle:\n",
    "            hist_ID_list.append(B + str(i))\n",
    "    return hist_ID_list\n",
    "\n",
    "# generate bin edges for SNR, CRFL, chastity\n",
    "def get_bin_edges(raw_data_path, chunksize, selected_cycle, num_bin, \n",
    "                  Good_File_Extension, Good_ID_Extension, divided_median):\n",
    "    filelist = []\n",
    "    for file in os.listdir(raw_data_path):\n",
    "        f = raw_data_path + file\n",
    "        filelist.append(f)\n",
    "\n",
    "    good_file_list = [s for s in filelist if Good_File_Extension in s]\n",
    "    good_ID_list = [s for s in filelist if s.endswith(Good_ID_Extension)]\n",
    "\n",
    "    hist_ID_list = make_hist_ID(selected_cycle)\n",
    "\n",
    "    binedge_dict = { i : [] for i in hist_ID_list}\n",
    "    count_dict = { i : [] for i in hist_ID_list}\n",
    "\n",
    "    for B in ['A', 'T', 'G', 'C']:\n",
    "        Good_chunk_list = []\n",
    "\n",
    "        for file in range(0, len(good_file_list)):\n",
    "\n",
    "            perfect = pd.read_csv(good_file_list[file], sep = \"\\t\", usecols = ['ID', 'Cycle', B], \n",
    "                                      chunksize = chunksize)\n",
    "            median = stat.median(pd.read_csv(good_file_list[file], \n",
    "                                      sep = \"\\t\", usecols = [B])[B])\n",
    "            dbperfect = pd.read_csv(good_ID_list[file],\n",
    "                                        sep = \"\\t\")\n",
    "\n",
    "            for data_chunk in perfect:  \n",
    "\n",
    "                data_chunk = data_chunk.sort_values(by=['ID', 'Cycle'])\n",
    "                db1 = dbperfect[dbperfect['ID'].isin(data_chunk.ID.unique().tolist())]\n",
    "\n",
    "                slist = \"\"\n",
    "                for s in db1.Sequence[0:len(db1.Sequence)]:\n",
    "                    slist = slist + str(s)\n",
    "\n",
    "                baselist = list(slist)\n",
    "\n",
    "                db2 = pd.DataFrame({'ID' : data_chunk.ID, 'Cycle' : data_chunk.Cycle, 'call_base' : baselist})  \n",
    "                db3 = pd.merge(data_chunk, db2, on = [\"ID\", \"Cycle\"], how = \"inner\") \n",
    "                db3 = db3[db3.call_base == B]\n",
    "                if divided_median == True:\n",
    "                    db3[B] = db3[B]/median\n",
    "\n",
    "                Good_chunk_list.append(db3[[B, \"Cycle\"]])\n",
    "\n",
    "        histdb = pd.concat(Good_chunk_list)\n",
    "\n",
    "        for cyc in selected_cycle:\n",
    "            histdb2 = histdb[histdb.Cycle == cyc]\n",
    "            histdb2 = histdb2[B].sort_values()\n",
    "            rowvalue = len(histdb2)/num_bin\n",
    "\n",
    "            row = 0\n",
    "            bin_edge = []\n",
    "            while row < num_bin:\n",
    "                row_num = round(rowvalue*row)\n",
    "                bin_edge.append(histdb2.iloc[row_num, ])\n",
    "                row = row + 1\n",
    "\n",
    "            bin_edge.append(histdb2.iloc[(len(histdb2))-1,])\n",
    "            binname = B+str(cyc)\n",
    "            binedge_dict[binname] =  bin_edge\n",
    "\n",
    "\n",
    "    hist_binedge_db = pd.DataFrame(binedge_dict)\n",
    "    return hist_binedge_db\n",
    "#list all files in a folder\n",
    "def list_files(path, pattern):\n",
    "    files_in_dir = []\n",
    "    # r=>root, d=>directories, f=>files\n",
    "    for r, d, f in os.walk(path):\n",
    "       for item in f:\n",
    "          if pattern in item:\n",
    "             files_in_dir.append(os.path.join(r, item))\n",
    "    return files_in_dir\n",
    "#get the bin edges using equal populations\n",
    "def Bin_Equal_Count(dblist, Base, cycle, num_bin = 50):\n",
    "    histdb = pd.concat(dblist).sort_values()\n",
    "    rowvalue = len(histdb)/num_bin\n",
    "    \n",
    "    row = 0\n",
    "    bin_edge = []\n",
    "    while row < num_bin:\n",
    "        rows = round(rowvalue*row)\n",
    "        bin_edge.append(histdb.iloc[rows, ])\n",
    "        row = row + 1\n",
    "        \n",
    "    bin_edge.append(histdb.iloc[(len(histdb))-1,])\n",
    "    binname = Base + str(cycle)\n",
    "    binedge_dict =  {binname : bin_edge}\n",
    "    return binedge_dict\n",
    "\n",
    "def dictionOne(L):\n",
    "    result = {}\n",
    "    for d in L:\n",
    "        result.update(d)\n",
    "    return result\n",
    "\n",
    "def histdict(db, cycles = [21, 50]):\n",
    "    hist_ID_list = []\n",
    "    for B in ['A', 'T', 'G', 'C']:\n",
    "        for i in cycles:\n",
    "            hist_ID_list.append(B + str(i))\n",
    "            \n",
    "    hist_bin = { i : [] for i in hist_ID_list}\n",
    "    \n",
    "    for ID in hist_ID_list:\n",
    "        x = db[ID].tolist()\n",
    "        for i in range(0, len(x)-1):\n",
    "            hist_bin[ID].append((x[i], x[i + 1]))\n",
    "    return hist_bin\n",
    "\n",
    "def make_folder(save_folder):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Get bin edges\n",
    "\n",
    "The following cell is for files and parameter input. Here are the description of each input:\n",
    "1. experiment_folder_path = the folder path where all the raw data is stored.\n",
    "2. save_folder = the bin edges files will be saved\n",
    "3. chunksize = due to large size data, the data will be processed in chunk.\n",
    "4. num_bin = numbers of bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder_path = \"D:/Binning_study_KW/Example_87x/Raw_data/\"\n",
    "save_folder = \"D:/Binning_study_KW/Example_87x/Bin_edges/\"\n",
    "chunksize = 900000\n",
    "selected_cycle = range(21, 151)\n",
    "num_bin = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is to generate bin edges for SNR normalized value and CRFL. \"Good_ID.tsv.SNR\" is for SNR and \"Good_ID.tsv.intensity\" is for CRFL. get_bin_edges function is to generate bin edges for these parameters. Due to SNR_maxminus is processed differently, get_bin_edges cannot generate bin edges for SNR_maxminus. The input of \"divided_median\" is the option for normalized the values by median. For SNR, the data is normalized but not for CRFL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SNR bin \n",
    "SNR_bin = get_bin_edges(raw_data_path = experiment_folder_path, \n",
    "              chunksize = chunksize, \n",
    "              selected_cycle = selected_cycle, \n",
    "              num_bin = num_bin,\n",
    "              Good_File_Extension = \"Good_ID.tsv.SNR\", \n",
    "              Good_ID_Extension = 'Good_ID.tsv', \n",
    "              divided_median = True)\n",
    "make_folder(save_folder)\n",
    "save_name = 'CRT87x_SNR_bin_edges_by_cycles'\n",
    "SNR_bin.to_csv(save_folder + save_name + '.tsv', sep='\\t', index=False)\n",
    "\n",
    "#Get CRFL bin\n",
    "CRFL_bin = get_bin_edges(raw_data_path = experiment_folder_path, \n",
    "              chunksize = chunksize, \n",
    "              selected_cycle = selected_cycle, \n",
    "              num_bin = num_bin,\n",
    "              Good_File_Extension = \"Good_ID.tsv.intensity\", \n",
    "              Good_ID_Extension = 'Good_ID.tsv', \n",
    "              divided_median = False)\n",
    "make_folder(save_folder)\n",
    "save_name = 'CRT87x_CRFL_bin_edges_by_cycles'\n",
    "CRFL_bin.to_csv(save_folder + save_name + '.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ths cell is to generate bin edges for SNR_maxminus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNR_max_bin\n",
    "filelist = []\n",
    "for file in os.listdir(experiment_folder_path):\n",
    "    f = experiment_folder_path + file\n",
    "    filelist.append(f)\n",
    "\n",
    "SNR_good_file_list = [s for s in filelist if \"Good_ID.tsv.SNR\" in s]\n",
    "SNR_good_ID_list = [s for s in filelist if s.endswith('Good_ID.tsv')]\n",
    "\n",
    "hist_ID_list = make_hist_ID(selected_cycle)\n",
    "binedge_dict = { i : [] for i in hist_ID_list}\n",
    "\n",
    "for B in ['A', 'T', 'G', 'C']:\n",
    "    Good_SNR_chunk_list = []\n",
    "    \n",
    "    for file in range(0, len(SNR_good_file_list)):\n",
    "        SNR_perfect = pd.read_csv(SNR_good_file_list[file], \n",
    "                          sep = \"\\t\", usecols = ['ID', 'Cycle', 'A', 'T', 'G', 'C'], \n",
    "                          chunksize= chunksize)\n",
    "        SNR_median_A = stat.median(pd.read_csv(SNR_good_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['A'])['A'])\n",
    "        SNR_median_T = stat.median(pd.read_csv(SNR_good_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['T'])['T'])\n",
    "        SNR_median_G = stat.median(pd.read_csv(SNR_good_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['G'])['G'])\n",
    "        SNR_median_C = stat.median(pd.read_csv(SNR_good_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['C'])['C'])\n",
    "        dbperfect = pd.read_csv(SNR_good_ID_list[file],\n",
    "                                    sep = \"\\t\")\n",
    "        \n",
    "        for data_chunk in SNR_perfect:  \n",
    "\n",
    "            data_chunk = data_chunk.sort_values(by=['ID', 'Cycle'])\n",
    "            \n",
    "            db1 = dbperfect[dbperfect['ID'].isin(data_chunk.ID.unique().tolist())]\n",
    "            \n",
    "            slist = \"\"\n",
    "            for s in db1.Sequence[0:len(db1.Sequence)]:\n",
    "                slist = slist + str(s)\n",
    "            \n",
    "            baselist = list(slist)\n",
    "            \n",
    "            db2 = pd.DataFrame({'ID' : data_chunk.ID, 'Cycle' : data_chunk.Cycle, \n",
    "                                'call_base' : baselist})   \n",
    "            db3 = pd.merge(data_chunk, db2, on = [\"ID\", \"Cycle\"], how = \"inner\") \n",
    "            db3['A'] = db3['A']/SNR_median_A\n",
    "            db3['T'] = db3['T']/SNR_median_T\n",
    "            db3['G'] = db3['G']/SNR_median_G\n",
    "            db3['C'] = db3['C']/SNR_median_C\n",
    "            db3 = db3[db3.call_base == B]\n",
    "            db3['SNR_value'] = db3[B]\n",
    "            m =db3.columns[2:6].to_numpy() == db3['call_base'].to_numpy()[:, None]\n",
    "            db3['maxminus'] = db3['SNR_value'] - db3.drop(columns= ['ID', 'Cycle','call_base', \n",
    "                                                                    'SNR_value'])\\\n",
    "                                                                         .mask(m).max(1)\n",
    "            \n",
    "            Good_SNR_chunk_list.append(db3[['maxminus', 'Cycle']])\n",
    "\n",
    "    histdb = pd.concat(Good_SNR_chunk_list).groupby(['Cycle'])\n",
    "\n",
    "    \n",
    "    for cyc in selected_cycle:\n",
    "        histdb2 = histdb.get_group(cyc)\n",
    "        histdb2 = histdb2['maxminus'].sort_values()\n",
    "        rowvalue = len(histdb2)/num_bin\n",
    "        \n",
    "        row = 0\n",
    "        bin_edge = []\n",
    "        while row < num_bin:\n",
    "            row_num = round(rowvalue*row)\n",
    "            bin_edge.append(histdb2.iloc[row_num, ])\n",
    "            row = row + 1\n",
    "\n",
    "        bin_edge.append(histdb2.iloc[(len(histdb2))-1,])\n",
    "        binname = B+str(cyc)\n",
    "        binedge_dict[binname] =  bin_edge\n",
    "    \n",
    "    \n",
    "hist_binedge_db = pd.DataFrame(binedge_dict)\n",
    "\n",
    "save_folder = save_folder\n",
    "make_folder(save_folder)\n",
    "\n",
    "hist_binedge_db.to_csv(save_folder + 'CRT87x_SNR_maxminus_bin_edges.tsv',\n",
    "       sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Generate Qtable\n",
    "\n",
    "After getting the bin edges, run the following two cells to generate the Qtable. Here are the path information:\n",
    "1. raw_data_path = it should be the same as the experiment_folder_path in the section (2a-2), where all the data is stored\n",
    "2. result_folder = it is the folder you want to store your results\n",
    "3. SNR_bin_edge_path = it is the SNR bin edges files generated using normalized value\n",
    "4. SNR_bin_edge_max_path = it is the SNR bin edges files generated using maxminus method\n",
    "5. SNR_bin_edge_max_path = it is the chastity bin edges files\n",
    "6. selected cycles = the cycles will be analyzed and in this particular study\n",
    "7. chunksize = number of row in a chunk; data is processed in chunk in order not to exceed the memory capacity.\n",
    "7. chastity_reduce_bin = the numbers of chastity bin will be compressed into one bin \n",
    "8. SNR_reduce_bin = the numbers of SNR bin will be compressed into one bin  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"D:/Binning_study_KW/Example_87x/Raw_data/\"\n",
    "result_folder = \"D:/Binning_study_KW/Example_87x/Results/\"\n",
    "SNR_bin_edge_path = \"D:/Binning_study_KW/Example_87x/Bin_edges/CRT87x_SNR_bin_edges_by_cycles.tsv.tsv\"\n",
    "SNR_bin_edge_max_path = \"D:/Binning_study_KW/Example_87x/Bin_edges/CRT87x_SNR_maxminus_bin_edges.tsv\"\n",
    "chasity_bin_edge_path = \"D:/Binning_study_KW/Example_87x/Bin_edges/CRT87x_CRFL_bin_edges_by_cycles.tsv.tsv\"\n",
    "selected_cycle = range(21, 151)\n",
    "chunksize = 900000\n",
    "chastity_reduce_bin = 0\n",
    "SNR_reduce_bin = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to generate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the raw data files\n",
    "filelist = []\n",
    "for fe in os.listdir(raw_data_path):\n",
    "    f =raw_data_path + fe\n",
    "    filelist.append(f)\n",
    "\n",
    "SNR_good_file_list = [s for s in filelist if \"Good_ID.tsv.SNRb\" in s]\n",
    "CRFL_good_file_list = [s for s in filelist if \"Good_ID.tsv.intensity\" in s]\n",
    "good_ID_list = [s for s in filelist if s.endswith('Good_ID.tsv')]\n",
    "SNR_bad_file_list = [s for s in filelist if \"Bad_ID.tsv.SNRb\" in s]\n",
    "CRFL_bad_file_list = [s for s in filelist if \"Bad_ID.tsv.intensity\" in s]\n",
    "bad_ID_list = [s for s in filelist if s.endswith('Bad_ID.tsv')]\n",
    "\n",
    "\n",
    "filename = list(map(lambda x: re.findall(r\"CRT87x_\\w\\d\\d\\.\\d\\d\", x), bad_ID_list))\n",
    "filename = [i[0] for i in filename]\n",
    "\n",
    "\n",
    "#laod bin edges data\n",
    "SNR_db = pd.read_csv(SNR_bin_edge_path, sep = \"\\t\")\n",
    "SNR_max_db = pd.read_csv(SNR_bin_edge_max_path, sep = \"\\t\")\n",
    "chas_db = pd.read_csv(chasity_bin_edge_path, sep = \"\\t\")\n",
    "\n",
    "SNR_hist_bin = {'SNR_value' : SNR_db, 'maxminus' : SNR_max_db}\n",
    "db_dict = {}\n",
    "\n",
    "#\n",
    "for file in range(0, len(good_ID_list)):\n",
    "    SNR_perfect = pd.read_csv(SNR_good_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['ID', 'Cycle', 'A', 'T', 'G', 'C'], \n",
    "                              chunksize = chunksize)\n",
    "    \n",
    "    SNR_median_A = stat.median(pd.read_csv(SNR_good_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['A'])['A'])\n",
    "    SNR_median_T = stat.median(pd.read_csv(SNR_good_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['T'])['T'])\n",
    "    SNR_median_G = stat.median(pd.read_csv(SNR_good_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['G'])['G'])\n",
    "    SNR_median_C = stat.median(pd.read_csv(SNR_good_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['C'])['C'])\n",
    "    \n",
    "    dbperfect = pd.read_csv(good_ID_list[file],\n",
    "                              sep = \"\\t\")\n",
    "    #read the good_SNR, good ID files by chunk, normalized the values, calculate the maxminus and append the data\n",
    "    # to Good_SNR_chunk_list\n",
    "    Good_SNR_chunk_list = []\n",
    "    for data_chunk in SNR_perfect:  \n",
    "    \n",
    "        data_chunk = data_chunk.sort_values(by=['ID', 'Cycle'])\n",
    "        db1 = dbperfect[dbperfect['ID'].isin(data_chunk.ID.unique().tolist())]\n",
    "        \n",
    "        slist = \"\"\n",
    "        for seq in db1.Sequence[0:len(db1.Sequence)]:\n",
    "            slist = slist + str(seq)\n",
    "        \n",
    "        baselist = list(slist)\n",
    "        \n",
    "        db2 = pd.DataFrame({'ID' : data_chunk.ID, 'Cycle' : data_chunk.Cycle, \n",
    "                            'call_base' : baselist})   \n",
    "        db3 = pd.merge(data_chunk, db2, on = [\"ID\", \"Cycle\"], how = \"inner\") \n",
    "        db3 = db3[['ID', 'Cycle', 'A', 'T', 'G', 'C', 'call_base']]\n",
    "        db3['A'] = db3['A']/SNR_median_A\n",
    "        db3['T'] = db3['T']/SNR_median_T\n",
    "        db3['G'] = db3['G']/SNR_median_G\n",
    "        db3['C'] = db3['C']/SNR_median_C\n",
    "        db3['SNR_value'] = db3.lookup(db3.index, db3['call_base'].astype(str))\n",
    "        m =db3.columns[2:6].to_numpy() == db3['call_base'].to_numpy()[:, None]\n",
    "        db3['maxminus'] = db3['SNR_value'] - db3.drop(columns= ['ID', 'Cycle','call_base', 'SNR_value'])\\\n",
    "                                                                    .mask(m).max(1)\n",
    "        db3 = db3[['ID', 'Cycle', 'call_base', 'SNR_value', 'maxminus']][db3.Cycle >20]\n",
    "        db3['type'] = 0 #type indicates aligned (0) or not aligned (1) \n",
    "        \n",
    "        Good_SNR_chunk_list.append(db3)\n",
    "    \n",
    "    \n",
    "    #read the good_CRFL, good ID files by chunk and append the data to Good_CRFL_chunk_list\n",
    "    CRFL_perfect = pd.read_csv(CRFL_good_file_list[file],       \n",
    "                              sep = \"\\t\", usecols = ['ID', 'Cycle', 'A', 'T', 'G', 'C'], \n",
    "                              chunksize=chunksize)\n",
    "    dbperfect = pd.read_csv(good_ID_list[file],\n",
    "                            sep = \"\\t\")\n",
    "    \n",
    "    Good_CRFL_chunk_list = []\n",
    "    \n",
    "    for data_chunk in CRFL_perfect:  \n",
    "    \n",
    "        data_chunk = data_chunk.sort_values(by=['ID', 'Cycle'])\n",
    "        db1 = dbperfect[dbperfect['ID'].isin(data_chunk.ID.unique().tolist())]\n",
    "        \n",
    "        slist = \"\"\n",
    "        for seq in db1.Sequence[0:len(db1.Sequence)]:\n",
    "            slist = slist + str(seq)\n",
    "        \n",
    "        baselist = list(slist)\n",
    "        \n",
    "        db2 = pd.DataFrame({'ID' : data_chunk.ID, 'Cycle' : data_chunk.Cycle, \n",
    "                            'call_base' : baselist})     \n",
    "        db3 = pd.merge(data_chunk, db2, on = [\"ID\", \"Cycle\"], how = \"inner\") \n",
    "        db3['CRFL_value'] = db3.lookup(db3.index, db3['call_base'].astype(str))\n",
    "        db3 = db3[['ID', 'Cycle', 'call_base', 'CRFL_value']][db3.Cycle >20]\n",
    "        db3['type'] = 0 0 #type indicates aligned (0) or not aligned (1)\n",
    "        \n",
    "        \n",
    "        Good_CRFL_chunk_list.append(db3)\n",
    "        \n",
    "    # read the bad_SNR, abd ID files by chunk, normalized the values, calculate the maxminus and append the aligned\n",
    "    # data to Bad_SNR_chunk_list_good and the not aligned data to Bad_SNR_chunk_list_bad\n",
    "    SNR_bad = pd.read_csv(SNR_bad_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['ID', 'Cycle', 'A', 'T', 'G', 'C'], \n",
    "                              chunksize = chunksize)\n",
    "    \n",
    "    SNR_median_A = stat.median(pd.read_csv(SNR_bad_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['A'])['A'])\n",
    "    SNR_median_T = stat.median(pd.read_csv(SNR_bad_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['T'])['T'])\n",
    "    SNR_median_G = stat.median(pd.read_csv(SNR_bad_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['G'])['G'])\n",
    "    SNR_median_C = stat.median(pd.read_csv(SNR_bad_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['C'])['C'])\n",
    "    \n",
    "    ID_db = pd.read_csv(bad_ID_list[file], sep = \"\\t\")\n",
    "    \n",
    "    Bad_SNR_chunk_list_good = []\n",
    "    Bad_SNR_chunk_list_bad =  []\n",
    "    \n",
    "    for data_chunk in SNR_bad:  \n",
    "    \n",
    "        data_chunk = data_chunk.sort_values(by=['ID', 'Cycle'])\n",
    "        db1 = pd.merge(data_chunk, ID_db[['ID', 'Miss']], \n",
    "                       on = [\"ID\"], how = \"inner\").sort_values(by=['ID', 'Cycle'])\n",
    "        \n",
    "        db1['A'] = db1['A']/SNR_median_A\n",
    "        db1['T'] = db1['T']/SNR_median_T\n",
    "        db1['G'] = db1['G']/SNR_median_G\n",
    "        db1['C'] = db1['C']/SNR_median_C\n",
    "       \n",
    "        ID_db2 = ID_db[ID_db['ID'].isin(db1.ID.unique().tolist())] \n",
    "        \n",
    "        slist = \"\"\n",
    "        for seq in ID_db2.Sequence[0:len(ID_db2.Sequence)]:\n",
    "            slist = slist + str(seq)\n",
    "        \n",
    "        baselist = list(slist)\n",
    "        \n",
    "        db2 = pd.DataFrame({'ID' : data_chunk.ID, 'Cycle' : data_chunk.Cycle, \n",
    "                           'call_base' : baselist})    \n",
    "        db3 = pd.merge(db1, db2, on = [\"ID\", \"Cycle\"], how = \"inner\") \n",
    "        db3 = db3[['ID', 'Cycle', 'A', 'T', 'G', 'C', 'Miss','call_base']]\n",
    "        db3['SNR_value'] = db3.lookup(db3.index, db3['call_base'].astype(str))\n",
    "        m =db3.columns[2:6].to_numpy() == db3['call_base'].to_numpy()[:, None]\n",
    "        db3['maxminus'] = db3['SNR_value'] - db3.drop(columns= ['ID', 'Cycle','call_base', 'SNR_value', 'Miss'])\\\n",
    "                                                                  .mask(m).max(1)\n",
    "        db4 = db3[db3.Cycle <= db3.Miss]\n",
    "        db4['type'] = db4.Cycle == db4.Miss\n",
    "        db4['type'] = db4['type'].astype(int)\n",
    "        \n",
    "        db5 = db4[['ID', 'Cycle', 'call_base', 'SNR_value', 'maxminus', 'type']][db4.Cycle >20]\n",
    "        \n",
    "        Bad_SNR_chunk_list_good.append(db5[db5.type == 0])\n",
    "        Bad_SNR_chunk_list_bad.append(db5[db5.type == 1])\n",
    "        \n",
    "    # read the bad_CRFL, bad ID files by chunk and append the aligned\n",
    "    # data to Bad_CRFL_chunk_list_good and the not aligned data to Bad_CRFL_chunk_list_bad\n",
    "    CRFL_Bad = pd.read_csv(CRFL_bad_file_list[file], \n",
    "                              sep = \"\\t\", usecols = ['ID', 'Cycle', 'A', 'T', 'G', 'C'], \n",
    "                              chunksize = chunksize)\n",
    "    \n",
    "    ID_db = pd.read_csv(bad_ID_list[file], sep = \"\\t\")\n",
    "        \n",
    "    Bad_CRFL_chunk_list_good = []\n",
    "    Bad_CRFL_chunk_list_bad =  []\n",
    "    \n",
    "    for data_chunk in CRFL_Bad:  \n",
    "    \n",
    "        data_chunk = data_chunk.sort_values(by=['ID', 'Cycle'])\n",
    "        db1 = pd.merge(data_chunk, ID_db[['ID', 'Miss']], \n",
    "                       on = [\"ID\"], how = \"inner\").sort_values(by=['ID', 'Cycle'])\n",
    "        \n",
    "        ID_db2 = ID_db[ID_db['ID'].isin(db1.ID.unique().tolist())] \n",
    "        \n",
    "        slist = \"\"\n",
    "        for seq in ID_db2.Sequence[0:len(ID_db2.Sequence)]:\n",
    "            slist = slist + str(seq)\n",
    "        \n",
    "        baselist = list(slist)\n",
    "        \n",
    "        db2 = pd.DataFrame({'ID' : data_chunk.ID, 'Cycle' : data_chunk.Cycle, \n",
    "                           'call_base' : baselist})   \n",
    "        db3 = pd.merge(db1, db2, on = [\"ID\", \"Cycle\"], how = \"inner\") \n",
    "        db3 = db3[['ID', 'Cycle', 'A', 'T', 'G', 'C', 'Miss','call_base']]\n",
    "        db3['CRFL_value'] = db3.lookup(db3.index, db3['call_base'].astype(str))\n",
    "        db4 = db3[db3.Cycle <= db3.Miss]\n",
    "        db4['type'] = db4.Cycle == db4.Miss\n",
    "        db4['type'] = db4['type'].astype(int)\n",
    "        \n",
    "        db5 = db4[['ID', 'Cycle', 'call_base', 'CRFL_value', 'type']][db4.Cycle >20]\n",
    "        \n",
    "        Bad_CRFL_chunk_list_good.append(db5[db5.type == 0])\n",
    "        Bad_CRFL_chunk_list_bad.append(db5[db5.type == 1])\n",
    "        \n",
    "    #append the aligned reads dataframe lists, concat to one data frame, group by cycle and call_base  \n",
    "    SNR_good_combine_list =  Good_SNR_chunk_list + Bad_SNR_chunk_list_good\n",
    "    CRFL_good_combine_list =  Good_CRFL_chunk_list + Bad_CRFL_chunk_list_good\n",
    "    SNR_good_combine_list = pd.concat(SNR_good_combine_list).groupby(['Cycle', 'call_base'])\n",
    "    CRFL_good_combine_list = pd.concat(CRFL_good_combine_list).groupby(['Cycle', 'call_base'])\n",
    "    #concat the not aligned reads dataframe list and then group by cycle and call_base \n",
    "    Bad_CRFL_chunk_list_bad = pd.concat(Bad_CRFL_chunk_list_bad).groupby(['Cycle', 'call_base'])\n",
    "    Bad_SNR_chunk_list_bad = pd.concat(Bad_SNR_chunk_list_bad).groupby(['Cycle', 'call_base'])\n",
    "    \n",
    "    goodkeylist = list(SNR_good_combine_list.groups.keys())\n",
    "    badkeylist = list(Bad_CRFL_chunk_list_bad.groups.keys())\n",
    "    # this loop is to bin the data and generate binning like Cycle#_C#_S#\n",
    "    for s in SNR_hist_bin.keys():  \n",
    "        for B in ['A', 'T', 'G', 'C']:\n",
    "            filterkey = list(it.compress(goodkeylist, [ B in i for i in goodkeylist]))\n",
    "            bin_id_list = []\n",
    "            for key in filterkey: \n",
    "\n",
    "                dfm = pd.merge(SNR_good_combine_list.get_group(key)[[s, 'ID', 'Cycle']], \n",
    "                               CRFL_good_combine_list.get_group(key), \n",
    "                               on = [\"ID\", \"Cycle\"], how = \"inner\")\n",
    "                df = dfm[['ID',  'Cycle', 'call_base',  'CRFL_value', s ,'type']]\n",
    "\n",
    "                hist_ID = key[1] + str(key[0])\n",
    "\n",
    "                 # create the bin ID column for chastity\n",
    "                hist_bin = chas_db[hist_ID][0: len(chas_db[hist_ID])-chastity_reduce_bin]\n",
    "                bin_edges =  hist_bin[0: (len(hist_bin)-1)].tolist()\n",
    "                bin_edges.append(float(\"inf\")) #adding inf to last bin to capture all the upper values\n",
    "                bin_edges[0] = -float(\"inf\") #adding -inf to first bin to capture all the lower values\n",
    "                values = [f'_C{i}' for i in range(1, len(bin_edges))]\n",
    "                df['chas_bin'] =  pd.cut(x = df['CRFL_value'], bins = bin_edges, labels= values)\n",
    "\n",
    "                # create the bin ID column for SNR\n",
    "                hist_bin = SNR_hist_bin[s][hist_ID][0: len(SNR_hist_bin[s][hist_ID]) - SNR_reduce_bin]\n",
    "                bin_edges =  hist_bin[0: (len(hist_bin)-1)].tolist()\n",
    "                bin_edges.append(float(\"inf\")) #adding inf to last bin to capture all the upper values\n",
    "                bin_edges[0] = -float(\"inf\") #adding -inf to first bin to capture all the lower values\n",
    "                values = [f'_S{i}' for i in range(1, len(bin_edges))]\n",
    "                df['SNR_bin'] = pd.cut(x = df[s], bins = bin_edges, labels= values)\n",
    "\n",
    "                df['bin_id'] = df.Cycle.astype(str) + df.chas_bin.astype(str) + df.SNR_bin.astype(str)\n",
    "                bin_id_list.append(df['bin_id'])\n",
    "\n",
    "            bin_db = pd.concat(bin_id_list)\n",
    "            bin_dict = bin_db.value_counts().to_dict()\n",
    "\n",
    "            ID_column = []\n",
    "            for cyc in selected_cycle:\n",
    "                for c in range(1, len(bin_edges)):\n",
    "                    for r in range(1, len(bin_edges)):\n",
    "                        ID = str(cyc) + \"_C\" + str(c) + \"_S\" + str(r)\n",
    "                        ID_column.append(ID)\n",
    "\n",
    "            count_column = [0] * len(ID_column)\n",
    "\n",
    "            Good_bin_db = pd.DataFrame({'ID' : ID_column, 'good_counts' : count_column})\n",
    "            Good_bin_db['good_counts'] = Good_bin_db['ID'].map(bin_dict).fillna(0)\n",
    "\n",
    "\n",
    "            #Bad bin\n",
    "            filterkey = list(it.compress(badkeylist, [ B in i for i in badkeylist]))\n",
    "            bin_id_list = []\n",
    "            for key in filterkey:     \n",
    "                dfm = pd.merge(Bad_SNR_chunk_list_bad.get_group(key)[[s, 'ID', 'Cycle']], \n",
    "                               Bad_CRFL_chunk_list_bad.get_group(key), \n",
    "                               on = [\"ID\", \"Cycle\"], how = \"inner\")\n",
    "                df = dfm[['ID',  'Cycle', 'call_base',  'CRFL_value', s ,'type']]\n",
    "\n",
    "                hist_ID = key[1] + str(key[0])\n",
    "\n",
    "                # create the bin ID column for chastity\n",
    "                hist_bin = chas_db[hist_ID][0: len(chas_db[hist_ID])- chastity_reduce_bin]\n",
    "                bin_edges =  hist_bin[0: (len(hist_bin)-1)].tolist()\n",
    "                bin_edges.append(float(\"inf\")) #adding inf to last bin to capture all the upper values\n",
    "                bin_edges[0] = -float(\"inf\") #adding -inf to first bin to capture all the lower values\n",
    "                values = [f'_C{i}' for i in range(1, len(bin_edges))]\n",
    "                df['chas_bin'] =  pd.cut(x = df['CRFL_value'], bins = bin_edges, labels= values)\n",
    "\n",
    "                # create the bin ID column for SNR\n",
    "                hist_bin = SNR_hist_bin[s][hist_ID][0: len(SNR_hist_bin[s][hist_ID])- SNR_reduce_bin]\n",
    "                bin_edges =  hist_bin[0: (len(hist_bin)-1)].tolist()\n",
    "                bin_edges.append(float(\"inf\")) #adding inf to last bin to capture all the upper values\n",
    "                bin_edges[0] = -float(\"inf\") #adding -inf to first bin to capture all the lower values\n",
    "                values = [f'_S{i}' for i in range(1, len(bin_edges))]\n",
    "                df['SNR_bin'] = pd.cut(x = df[s], bins = bin_edges, labels= values)\n",
    "\n",
    "                df['bin_id'] = df.Cycle.astype(str) + df.chas_bin.astype(str) + df.SNR_bin.astype(str)\n",
    "                bin_id_list.append(df['bin_id'])\n",
    "\n",
    "            bin_db = pd.concat(bin_id_list)\n",
    "            bin_dict = bin_db.value_counts().to_dict()\n",
    "\n",
    "            ID_column = []\n",
    "            for cyc in selected_cycle:\n",
    "                for c in range(1, len(bin_edges)):\n",
    "                    for r in range(1, len(bin_edges)):\n",
    "                        ID = str(cyc) + \"_C\" + str(c) + \"_S\" + str(r)\n",
    "                        ID_column.append(ID)\n",
    "\n",
    "            count_column = [0] * len(ID_column)\n",
    "\n",
    "            Bad_bin_db = pd.DataFrame({'ID' : ID_column, 'bad_counts' : count_column})\n",
    "            Bad_bin_db['bad_counts'] = Bad_bin_db['ID'].map(bin_dict).fillna(0)\n",
    "\n",
    "            result_folder = result_folder\n",
    "            make_folder(result_folder)\n",
    "\n",
    "            list_name = filename[file] + '_' + s + '_' + 'base_' + B \n",
    "\n",
    "            save_bin_db = pd.merge(Bad_bin_db, Good_bin_db, \n",
    "                           on = [\"ID\"], how = \"inner\")\n",
    "\n",
    "            db_dict[list_name] = save_bin_db\n",
    "    \n",
    "            \n",
    "# This section is to process db_list and save the files. Four types of files [\"bad_counts\", \"good_counts\", \"error\", \"qscore\"]\n",
    "# at each cycle and bases will be saved. The file will be saved in the format of SNR bins as the row index and chastity bins\n",
    "# as the column headings\n",
    "chastity_name = ['chastity' + str(x) for x in range(1, len(bin_edges))]\n",
    "SNR_name = ['SNR' + str(x) for x in range(1, len(bin_edges))]\n",
    "\n",
    "for SNR_method in SNR_hist_bin.keys():\n",
    "    for base in [\"base_A\", \"base_T\", \"base_G\", \"base_C\"]:\n",
    "        L1 = [s for s in db_dict.keys() if SNR_method in s and base in s]\n",
    "        L2 = []\n",
    "        for file in L1:\n",
    "            L2.append(db_dict[file])\n",
    "        db = pd.concat(L2).groupby(by=[\"ID\"]).sum().reset_index().sort_values(by=['ID'])\n",
    "        db['ID'] = pd.Categorical(db['ID'], ordered=True, categories= ns.natsorted(db['ID'].unique()))\n",
    "        db = db.sort_values('ID')\n",
    "        db[['Cycle','chas_bin', 'SNR_bin']] = db.ID.str.split(\"_\",expand=True,)\n",
    "        db['error'] = db.bad_counts / (db.good_counts + db.bad_counts )\n",
    "        db['qscore'] = round((10 * -np.log10(db.error)), 0)\n",
    "        for cyc in selected_cycle:\n",
    "            for name in [\"bad_counts\", \"good_counts\", \"error\", \"qscore\"]:\n",
    "                db1 = db[db.Cycle == str(cyc)][name]\n",
    "                n = len(bin_edges)-1\n",
    "                outList = []\n",
    "                for i in range(n, len(db1) + n, n):\n",
    "                    outList.append(db1[i-n:i])\n",
    "\n",
    "                db2 = pd.DataFrame(np.array(outList), columns=chastity_name,\n",
    "                                  index = SNR_name)\n",
    "                savefolder = result_folder + SNR_method + '/' + \"cycle_\" + str(cyc) + \"_\" +  base +  \"/\" \n",
    "                make_folder(savefolder)\n",
    "                savefilename = savefolder + SNR_method + \"_\" + base + \"_cycle_\" + str(cyc) + \"_\" + name + '.csv'\n",
    "                db2.to_csv(savefilename, sep=',', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
